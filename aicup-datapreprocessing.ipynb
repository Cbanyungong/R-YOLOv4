{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aicup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwk2LkdpVSwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72402574-f7e7-46df-e3c1-1c204b75f15b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBnJbqfImIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "a0a1d889-f1b8-4316-d1ee-231295c5bb9a"
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/f4/9f93f06dd2c57c7cd7aa515ffbf9fcfd8a084b92285732289f4a5696dd91/transformers-3.2.0-py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 31.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=1bd8cf234e94eff9c4ac4053db3702a33d0b65dbc63ac20526e17948523c32e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyBSIoZUaiMD",
        "colab_type": "text"
      },
      "source": [
        "> init_seeds    --> 固定random number 以利判斷訓練好壞 </br>\n",
        "> loadInputFile --> 整理資料 </br>\n",
        "> CRFFormatData --> 編排BIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL5kAxFOWCQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_seeds(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    # Sets the seed for generating random numbers.\n",
        "    torch.manual_seed(seed)\n",
        "    # Sets the seed for generating random numbers for the current GPU.\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # Sets the seed for generating random numbers on all GPUs.\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def loadInputFile(path):\n",
        "    trainingset = list()  # store trainingset [content,content,...]\n",
        "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
        "    mentions = dict()  # store mentions[mention] = Type\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
        "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
        "    for data in datas:\n",
        "        data=data.split('\\n')\n",
        "        content=data[0]\n",
        "        trainingset.append(content)\n",
        "        annotations=data[1:]\n",
        "        for annot in annotations[1:]:\n",
        "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
        "            position.extend(annot)\n",
        "            mentions[annot[3]]=annot[4]\n",
        "    \n",
        "    return trainingset, position, mentions\n",
        "\n",
        "def CRFFormatData(trainingset, position):\n",
        "    outputfile = []\n",
        "\n",
        "    # output file lines\n",
        "    count = 0 # annotation counts in each content\n",
        "    tagged = list()\n",
        "    for article_id in range(len(trainingset)):\n",
        "        trainingset_split = list(trainingset[article_id])\n",
        "        while '' or ' ' in trainingset_split:\n",
        "            if '' in trainingset_split:\n",
        "                trainingset_split.remove('')\n",
        "            else:\n",
        "                trainingset_split.remove(' ')\n",
        "        start_tmp = 0\n",
        "        for position_idx in range(0,len(position),5):\n",
        "            if int(position[position_idx]) == article_id:\n",
        "                count += 1\n",
        "                if count == 1:\n",
        "                    start_pos = int(position[position_idx+1])\n",
        "                    end_pos = int(position[position_idx+2])\n",
        "                    entity_type=position[position_idx+4]\n",
        "                    if start_pos == 0:\n",
        "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            # BIO states\n",
        "                            if token_idx == 0:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                            \n",
        "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                            outputfile.append(output_str)\n",
        "\n",
        "                    else:\n",
        "                        token = list(trainingset[article_id][0:start_pos])\n",
        "                        whole_token = trainingset[article_id][0:start_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            \n",
        "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "                            outputfile.append(output_str)\n",
        "\n",
        "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            # BIO states\n",
        "                            if token[0] == '':\n",
        "                                if token_idx == 1:\n",
        "                                    label = 'B-'+entity_type\n",
        "                                else:\n",
        "                                    label = 'I-'+entity_type\n",
        "                            else:\n",
        "                                if token_idx == 0:\n",
        "                                    label = 'B-'+entity_type\n",
        "                                else:\n",
        "                                    label = 'I-'+entity_type\n",
        "\n",
        "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                            outputfile.append(output_str)\n",
        "\n",
        "                    start_tmp = end_pos\n",
        "                else:\n",
        "                    start_pos = int(position[position_idx+1])\n",
        "                    end_pos = int(position[position_idx+2])\n",
        "                    entity_type=position[position_idx+4]\n",
        "                    if start_pos<start_tmp:\n",
        "                        continue\n",
        "                    else:\n",
        "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
        "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "                            outputfile.append(output_str)\n",
        "\n",
        "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                    for token_idx in range(len(token)):\n",
        "                        if len(token[token_idx].replace(' ','')) == 0:\n",
        "                            continue\n",
        "                        # BIO states\n",
        "                        if token[0] == '':\n",
        "                            if token_idx == 1:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                        else:\n",
        "                            if token_idx == 0:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                        \n",
        "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                        outputfile.append(output_str)\n",
        "                    start_tmp = end_pos\n",
        "\n",
        "        token = list(trainingset[article_id][start_tmp:])\n",
        "        whole_token = trainingset[article_id][start_tmp:]\n",
        "\n",
        "        for token_idx in range(len(token)):\n",
        "            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                continue\n",
        "\n",
        "            \n",
        "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "            outputfile.append(output_str)\n",
        "\n",
        "        count = 0\n",
        "    \n",
        "        # 一個article的結束\n",
        "        output_str = '\\n'\n",
        "        outputfile.append(output_str)\n",
        "        ID = trainingset[article_id]\n",
        "\n",
        "        if article_id%10 == 0:\n",
        "            print('Total complete articles:', article_id)\n",
        "\n",
        "    return outputfile"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnhY_0UNbHVM",
        "colab_type": "text"
      },
      "source": [
        "* 因為直接使用Bert tokenizer對整個句子做tokneizer的話 專業用語的英文字母會被變成一個Mask\n",
        "> 例如： ALP 在BIO格式中應該要為 O O O、然而Bert tokenize會直接把ALP變成[UNK]、造成跟BIO中的格式長度不相同。</br>\n",
        "\n",
        "* 因此，用seperate_words_bio將已經整理好了CRF來將words跟bio label分割成兩個不同的List, 同時將每個字後面增加一個空格，如此一來Bert tokenizer就不會直接把整個英文字母變成一個Mask 而是將每個英文字母都變成一個Mask了。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQD5EfDGvkhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seperate_words_bio(data):\n",
        "    token = []\n",
        "    bio_label = []\n",
        "    token_temp = \"\"\n",
        "    bio_temp = []\n",
        "    character = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    for row in data:\n",
        "        if row == '\\n' or len(token_temp) >= 1000:\n",
        "            token.append(token_temp)\n",
        "            bio_label.append(bio_temp)\n",
        "            token_temp = \"\"\n",
        "            bio_temp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            token_temp = token_temp + row[0] + \" \"\n",
        "            bio_temp.append(row[1])\n",
        "\n",
        "    return token, bio_label"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grx6PpXFWQMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NerDataset(Dataset):\n",
        "    def __init__(self, mode, sen_len, trainingset, position, seperate_words_bio, bert_tokenizer):\n",
        "        self.mode = mode\n",
        "        self.data = CRFFormatData(trainingset, position)\n",
        "        self.maxlen = sen_len\n",
        "        self.seperate_words_bio = seperate_words_bio  # 自己寫的tokenizer\n",
        "        self.bert_tokenizer = bert_tokenizer\n",
        "\n",
        "        # 建立data的tokens還有對應到的BIO\n",
        "        self.tokens, self.bio_label = self.seperate_words_bio(self.data)\n",
        "        self.len = len(self.tokens) # 有幾個input\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        origin_text = self.tokens[index]\n",
        "\n",
        "        text_a = self.bert_tokenizer.tokenize(self.tokens[index])\n",
        "        text_b = None  # for natural language inference、我們的任務不用進行文章推論因此不用輸入bert input的第二句\n",
        "        label = self.bio_label[index]\n",
        "\n",
        "        # 將整個 token 序列轉換成索引序列後變成tensor\n",
        "        word_pieces = [\"[CLS]\"]\n",
        "        word_pieces += text_a[:self.maxlen] + [\"[SEP]\"]\n",
        "        len_a = len(word_pieces)\n",
        "        ids = self.bert_tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "\n",
        "        # labal不用建立tensor因為不會input到Bert\n",
        "        label_out = [\"[strart]\"]\n",
        "        label_out += label[:self.maxlen] + [\"[end]\"]\n",
        "\n",
        "        # 我們的task只有一個句子，所以segments_tensor的部分會全部給1\n",
        "        segments_tensor = torch.tensor([1] * len_a, dtype=torch.long)\n",
        "\n",
        "        return tokens_tensor, segments_tensor, label_out, origin_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivERX9o3iOxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 建立pytorch dataloader 來一次取一個batch的資料\n",
        "# collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
        "# 截長補短後要限制attention只注意非pad 的部分\n",
        "def create_mini_batch(samples):\n",
        "    \"\"\"\n",
        "    :param: NerDataset的回傳值\n",
        "            - tokens_tensor  : samples[0]\n",
        "            - segments_tensor: samples[1]\n",
        "            - label_out   : samples[2]\n",
        "            - origin_text    : samples[3]\n",
        "    :return: 餵給 BERT 時會需要的 3 個 tensors\n",
        "            - tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
        "            - segments_tensors: (batch_size, max_seq_len_in_batch)\n",
        "            - masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
        "    \"\"\"\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "\n",
        "    # zero pad到該batch下最長的長度\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
        "\n",
        "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
        "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
        "\n",
        "    return tokens_tensors, segments_tensors, masks_tensors"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mypCpJrnmYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "sen_len = 500\n",
        "batch_size = 16"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kod8AUM4de5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "161c0e50-b973-4b5b-9470-be856a05c029"
      },
      "source": [
        "# 資料前處理，讀出檔案\n",
        "file_path='/content/drive/My Drive/aicup/SampleData_deid.txt'\n",
        "trainingset, position, mentions = loadInputFile(file_path)\n",
        "\n",
        "# 用pytorch Dataset\n",
        "init_seeds()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
        "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "trainset = NerDataset(\"train\", sen_len, trainingset, position, seperate_words_bio=seperate_words_bio, bert_tokenizer=tokenizer)\n",
        "\n",
        "# split val from trainset\n",
        "val_size = int(trainset.__len__() * 0.05)  # 切出5%筆當validation\n",
        "trainset, valset = random_split(trainset, [trainset.__len__() - val_size, val_size])\n",
        "print('trainset size:', trainset.__len__())\n",
        "print('valset size:', valset.__len__())"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total complete articles: 0\n",
            "Total complete articles: 10\n",
            "Total complete articles: 20\n",
            "trainset size: 103\n",
            "valset size: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCbx3Wx9nd70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(trainset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=batch_size, collate_fn=create_mini_batch, shuffle=False)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc7dBwsHkSMK",
        "colab_type": "text"
      },
      "source": [
        "##### 上面是tokenized words\n",
        "##### 中間是Bert的word embeddings\n",
        "##### 下面是對應到的BIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3umIY1Hh6z8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "86dc7d31-936f-4618-dabb-0d3884a28a35"
      },
      "source": [
        "tokens_tensor, segments_tensor, label, origin_text = trainset[0]"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '醫', '師', '：', '你', '有', '做', '超', '音', '波', '嘛', '，', '那', '我', '們', '來', '看', '報', '告', '，', '有', '些', '部', '分', '有', '紅', '字', '耶', '。', '民', '眾', '：', '紅', '字', '是', '甚', '麼', '意', '思', '？', '醫', '師', '：', '就', '是', '肝', '功', '能', '有', '比', '較', '高', '，', '肝', '功', '能', '6', '8', '，', '就', '是', '這', '個', '[UNK]', '[UNK]', '[UNK]', '是', '6', '8', '，', '這', '樣', '比', '較', '高', '，', '正', '常', '應', '是', '5', '0', '以', '下', '，', '另', '外', '就', '是', '你', '之', '前', '說', '你', '有', '[UNK]', '肝', '，', '但', '是', '你', '[UNK]', '肝', '已', '經', '好', '了', '耶', '。', '民', '眾', '：', '它', '會', '自', '動', '修', '復', '阿', '。', '醫', '師', '：', '你', '有', '抗', '體', '了', '阿', '，', '所', '以', '你', '[UNK]', '肝', '已', '經', '沒', '帶', '原', '了', '耶', '。', '民', '眾', '：', '我', '以', '前', '被', '關', '的', '時', '候', '，', '就', '有', '在', '固', '定', '驗', '血', '，', '那', '時', '候', '說', '有', '[UNK]', '肝', '。', '醫', '師', '：', '阿', '你', '現', '在', '已', '經', '有', '保', '護', '的', '抗', '體', '了', '。', '但', '是', '你', '現', '在', '有', '[UNK]', '肝', '。', '民', '眾', '：', '[UNK]', '肝', '？', '醫', '師', '：', '[UNK]', '型', '肝', '炎', '，', '[UNK]', '、', '[UNK]', '、', '[UNK]', '的', '[UNK]', '型', '肝', '炎', '。', '這', '部', '分', '你', '沒', '有', '檢', '查', '過', '對', '不', '對', '。', '民', '眾', '：', '沒', '有', '耶', '，', '但', '是', '一', '般', '被', '關', '的', '時', '候', '都', '會', '驗', '血', '，', '還', '是', '他', '只', '會', '驗', '[UNK]', '肝', '沒', '有', '驗', '[UNK]', '肝', '？', '醫', '師', '：', '一', '般', '都', '會', '這', '樣', '耶', '。', '民', '眾', '：', '但', '是', '那', '時', '候', '他', '只', '跟', '我', '說', '[UNK]', '肝', '，', '沒', '[UNK]', '肝', '。', '醫', '師', '：', '那', '你', '那', '是', '甚', '麼', '時', '候', '做', '的', '檢', '查', '？', '民', '眾', '：', '很', '久', '以', '前', '了', '。', '醫', '師', '：', '我', '建', '議', '這', '個', '[UNK]', '肝', '要', '檢', '查', '一', '下', '。', '民', '眾', '：', '要', '怎', '麼', '檢', '查', '？', '要', '抽', '血', '嗎', '？', '醫', '師', '：', '要', '再', '做', '進', '一', '步', '更', '精', '密', '的', '檢', '查', '，', '看', '[UNK]', '型', '肝', '炎', '的', '基', '因', '，', '這', '樣', '比', '較', '準', '。', '然', '後', '另', '外', '就', '是', '我', '建', '議', '你', '做', '是', '因', '為', '現', '在', '[UNK]', '肝', '目', '前', '有', '藥', '物', '可', '以', '治', '療', '。', '民', '眾', '：', '恩', '對', '。', '醫', '師', '：', '你', '知', '道', '嘛', '，', '[UNK]', '肝', '因', '為', '有', '符', '合', '歐', '洲', '的', '標', '準', '，', '政', '府', '會', '幫', '你', '出', '錢', '。', '民', '眾', '：', '喔', '～', '醫', '師', '：', '可', '以', '申', '請', '看', '看', '。', '民', '眾', '：', '喔', '～', '醫', '師', '：', '阿', '這', '個', '是', '意', '外', '發', '現', '啦', '。', '民', '眾', '：', '喔', '～', '醫', '師', '：', '對', '阿', '，', '所', '以', '我', '們', '就', '會', '另', '[SEP]']\n",
            "[101, 7015, 2374, 8038, 872, 3300, 976, 6631, 7509, 3797, 1658, 8024, 6929, 2769, 947, 889, 4692, 1841, 1440, 8024, 3300, 763, 6956, 1146, 3300, 5148, 2099, 5456, 511, 3696, 4707, 8038, 5148, 2099, 3221, 4493, 7938, 2692, 2590, 8043, 7015, 2374, 8038, 2218, 3221, 5498, 1216, 5543, 3300, 3683, 6733, 7770, 8024, 5498, 1216, 5543, 127, 129, 8024, 2218, 3221, 6857, 943, 100, 100, 100, 3221, 127, 129, 8024, 6857, 3564, 3683, 6733, 7770, 8024, 3633, 2382, 2746, 3221, 126, 121, 809, 678, 8024, 1369, 1912, 2218, 3221, 872, 722, 1184, 6303, 872, 3300, 100, 5498, 8024, 852, 3221, 872, 100, 5498, 2347, 5195, 1962, 749, 5456, 511, 3696, 4707, 8038, 2124, 3298, 5632, 1240, 934, 2541, 7350, 511, 7015, 2374, 8038, 872, 3300, 2834, 7768, 749, 7350, 8024, 2792, 809, 872, 100, 5498, 2347, 5195, 3760, 2380, 1333, 749, 5456, 511, 3696, 4707, 8038, 2769, 809, 1184, 6158, 7302, 4638, 3229, 952, 8024, 2218, 3300, 1762, 1743, 2137, 7710, 6117, 8024, 6929, 3229, 952, 6303, 3300, 100, 5498, 511, 7015, 2374, 8038, 7350, 872, 4412, 1762, 2347, 5195, 3300, 924, 6362, 4638, 2834, 7768, 749, 511, 852, 3221, 872, 4412, 1762, 3300, 100, 5498, 511, 3696, 4707, 8038, 100, 5498, 8043, 7015, 2374, 8038, 100, 1798, 5498, 4142, 8024, 100, 510, 100, 510, 100, 4638, 100, 1798, 5498, 4142, 511, 6857, 6956, 1146, 872, 3760, 3300, 3596, 3389, 6882, 2205, 679, 2205, 511, 3696, 4707, 8038, 3760, 3300, 5456, 8024, 852, 3221, 671, 5663, 6158, 7302, 4638, 3229, 952, 6963, 3298, 7710, 6117, 8024, 6917, 3221, 800, 1372, 3298, 7710, 100, 5498, 3760, 3300, 7710, 100, 5498, 8043, 7015, 2374, 8038, 671, 5663, 6963, 3298, 6857, 3564, 5456, 511, 3696, 4707, 8038, 852, 3221, 6929, 3229, 952, 800, 1372, 6656, 2769, 6303, 100, 5498, 8024, 3760, 100, 5498, 511, 7015, 2374, 8038, 6929, 872, 6929, 3221, 4493, 7938, 3229, 952, 976, 4638, 3596, 3389, 8043, 3696, 4707, 8038, 2523, 719, 809, 1184, 749, 511, 7015, 2374, 8038, 2769, 2456, 6359, 6857, 943, 100, 5498, 6206, 3596, 3389, 671, 678, 511, 3696, 4707, 8038, 6206, 2582, 7938, 3596, 3389, 8043, 6206, 2853, 6117, 1621, 8043, 7015, 2374, 8038, 6206, 1086, 976, 6868, 671, 3635, 3291, 5125, 2166, 4638, 3596, 3389, 8024, 4692, 100, 1798, 5498, 4142, 4638, 1825, 1728, 8024, 6857, 3564, 3683, 6733, 3976, 511, 4197, 2527, 1369, 1912, 2218, 3221, 2769, 2456, 6359, 872, 976, 3221, 1728, 4158, 4412, 1762, 100, 5498, 4680, 1184, 3300, 5973, 4289, 1377, 809, 3780, 4615, 511, 3696, 4707, 8038, 2617, 2205, 511, 7015, 2374, 8038, 872, 4761, 6887, 1658, 8024, 100, 5498, 1728, 4158, 3300, 5016, 1394, 3627, 3828, 4638, 3560, 3976, 8024, 3124, 2424, 3298, 2396, 872, 1139, 7092, 511, 3696, 4707, 8038, 1595, 8080, 7015, 2374, 8038, 1377, 809, 4509, 6313, 4692, 4692, 511, 3696, 4707, 8038, 1595, 8080, 7015, 2374, 8038, 7350, 6857, 943, 3221, 2692, 1912, 4634, 4412, 1568, 511, 3696, 4707, 8038, 1595, 8080, 7015, 2374, 8038, 2205, 7350, 8024, 2792, 809, 2769, 947, 2218, 3298, 1369, 102]\n",
            "['[strart]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-med_exam', 'I-med_exam', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-med_exam', 'I-med_exam', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[end]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFU31Fqtvkq",
        "colab_type": "text"
      },
      "source": [
        "##### 分別要餵給BertModel的三個input應該就是下面這樣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr4Z_g5GntCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7db73fae-b1f3-42f0-b212-249584c7ba1e"
      },
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "      print(i)\n",
        "      tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
        "      print(masks_tensors)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 101, 1962, 8024,  ..., 5517, 7128,  102],\n",
            "        [ 101,  100,  100,  ..., 1343, 6525,  102],\n",
            "        [ 101,  943, 5052,  ..., 2205, 2205,  102],\n",
            "        ...,\n",
            "        [ 101, 2253, 8038,  ...,  784, 7939,  102],\n",
            "        [ 101,  800,  738,  ..., 5839,  511,  102],\n",
            "        [ 101, 4707, 8038,  ...,    0,    0,    0]])\n",
            "0\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "tensor([[ 101, 7015, 2374,  ..., 1914, 5857,  102],\n",
            "        [ 101, 8038, 1962,  ..., 2218, 1391,  102],\n",
            "        [ 101, 3221, 3173,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1762, 6857,  ...,    0,    0,    0],\n",
            "        [ 101, 6123, 2802,  ..., 2582, 7938,  102],\n",
            "        [ 101, 4707, 8038,  ...,    0,    0,    0]])\n",
            "1\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "tensor([[ 101, 6303, 3760,  ...,  511,  943,  102],\n",
            "        [ 101, 2374, 8038,  ..., 2853, 6117,  102],\n",
            "        [ 101, 3696, 4707,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 7015, 2374,  ..., 1059, 6716,  102],\n",
            "        [ 101, 2527, 6818,  ...,  679, 3221,  102],\n",
            "        [ 101, 8024,  800,  ..., 4638, 3300,  102]])\n",
            "2\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])\n",
            "tensor([[ 101, 2251, 7481,  ..., 8043, 3696,  102],\n",
            "        [ 101, 7015, 2374,  ..., 1762, 6752,  102],\n",
            "        [ 101, 8024, 1922,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1377, 3221,  ..., 2204, 3298,  102],\n",
            "        [ 101,  511, 1391,  ...,    0,    0,    0],\n",
            "        [ 101, 6260,  872,  ..., 6929, 2218,  102]])\n",
            "3\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])\n",
            "tensor([[ 101, 1798,  511,  ...,  511, 3696,  102],\n",
            "        [ 101,  511, 7015,  ..., 6963, 3298,  102],\n",
            "        [ 101, 3696, 4707,  ..., 6511, 3085,  102],\n",
            "        ...,\n",
            "        [ 101, 2374, 8038,  ..., 1595,  511,  102],\n",
            "        [ 101,  943, 5052,  ..., 1391, 2407,  102],\n",
            "        [ 101, 2198, 1595,  ...,    0,    0,    0]])\n",
            "4\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "tensor([[ 101, 8024,  679,  ...,    0,    0,    0],\n",
            "        [ 101, 1762, 2228,  ..., 1348, 3298,  102],\n",
            "        [ 101, 2157, 2253,  ..., 1886,  738,  102],\n",
            "        ...,\n",
            "        [ 101,  872, 1962,  ..., 1440,  511,  102],\n",
            "        [ 101, 3696, 4707,  ..., 6206, 5481,  102],\n",
            "        [ 101, 3696, 4707,  ..., 3696, 4707,  102]])\n",
            "5\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])\n",
            "tensor([[ 101, 6158, 2697,  ..., 8038, 2205,  102],\n",
            "        [ 101, 1213, 2339,  ..., 1644,  511,  102],\n",
            "        [ 101, 3717, 7967,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1993, 1872,  ..., 5584,  510,  102],\n",
            "        [ 101, 7015, 2374,  ...,  679, 5653,  102],\n",
            "        [ 101, 7015, 2374,  ..., 4995,  511,  102]])\n",
            "6\n",
            "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}